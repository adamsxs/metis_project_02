{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis of Storm & Cost Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 728 entries, 0 to 727\n",
      "Data columns (total 23 columns):\n",
      "damage_USD        728 non-null float64\n",
      "name              728 non-null object\n",
      "stormID           728 non-null object\n",
      "areas_affected    728 non-null object\n",
      "dates_active      728 non-null object\n",
      "max_storm_cat     728 non-null int64\n",
      "deaths            728 non-null int64\n",
      "year              728 non-null object\n",
      "duration          728 non-null float64\n",
      "lat_delta         728 non-null float64\n",
      "lon_delta         728 non-null float64\n",
      "wind_v_max        728 non-null float64\n",
      "wind_v_med        728 non-null float64\n",
      "p_min             728 non-null float64\n",
      "p_med             622 non-null float64\n",
      "34kt_r_max        230 non-null float64\n",
      "34kt_r_med        230 non-null float64\n",
      "50kt_r_max        230 non-null float64\n",
      "50kt_r_med        230 non-null float64\n",
      "64kt_r_max        230 non-null float64\n",
      "64kt_r_med        230 non-null float64\n",
      "damage_imputed    728 non-null int64\n",
      "landfall          728 non-null int64\n",
      "dtypes: float64(14), int64(4), object(5)\n",
      "memory usage: 136.5+ KB\n"
     ]
    }
   ],
   "source": [
    "path = './'\n",
    "\n",
    "#file = 'storms_with_effect.pkl' #old dataset\n",
    "file = 'storms_merged.pkl'\n",
    "\n",
    "hur_cost = pd.read_pickle(path+file)\n",
    "\n",
    "hur_cost.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728, 18)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating masks for later use\n",
    "no_effect = (hur_cost.areas_affected == 'None')\n",
    "effect = (hur_cost.areas_affected != 'None')\n",
    "\n",
    "# Columns by type\n",
    "non_numeric = ['name', 'stormID', 'areas_affected', 'dates_active', 'year']\n",
    "\n",
    "numeric = ['damage_USD','max_storm_cat', 'deaths', 'duration',\n",
    "           'lat_delta', 'lon_delta', 'wind_v_max','wind_v_med',\n",
    "           'p_min', 'p_med', '34kt_r_max', '34kt_r_med', '50kt_r_max',\n",
    "           '50kt_r_med', '64kt_r_max', '64kt_r_med', 'damage_imputed', 'landfall',]\n",
    "\n",
    "cols_radii = ['34kt_r_max', '34kt_r_med', '50kt_r_max',\n",
    "           '50kt_r_med', '64kt_r_max', '64kt_r_med']\n",
    "\n",
    "hur_cost[numeric].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>damage_USD</th>\n",
       "      <th>deaths</th>\n",
       "      <th>p_min</th>\n",
       "      <th>max_storm_cat</th>\n",
       "      <th>wind_v_max</th>\n",
       "      <th>p_med</th>\n",
       "      <th>64kt_r_med</th>\n",
       "      <th>wind_v_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>damage_USD</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.506317</td>\n",
       "      <td>-0.464473</td>\n",
       "      <td>0.408222</td>\n",
       "      <td>0.400592</td>\n",
       "      <td>-0.367360</td>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.329425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deaths</th>\n",
       "      <td>0.506317</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.303482</td>\n",
       "      <td>0.264166</td>\n",
       "      <td>0.268409</td>\n",
       "      <td>-0.221883</td>\n",
       "      <td>0.198403</td>\n",
       "      <td>0.214257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_min</th>\n",
       "      <td>-0.464473</td>\n",
       "      <td>-0.303482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.938941</td>\n",
       "      <td>-0.959126</td>\n",
       "      <td>0.819427</td>\n",
       "      <td>-0.627438</td>\n",
       "      <td>-0.798070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_storm_cat</th>\n",
       "      <td>0.408222</td>\n",
       "      <td>0.264166</td>\n",
       "      <td>-0.938941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975015</td>\n",
       "      <td>-0.745828</td>\n",
       "      <td>0.589159</td>\n",
       "      <td>0.773304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind_v_max</th>\n",
       "      <td>0.400592</td>\n",
       "      <td>0.268409</td>\n",
       "      <td>-0.959126</td>\n",
       "      <td>0.975015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.765522</td>\n",
       "      <td>0.584951</td>\n",
       "      <td>0.794760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_med</th>\n",
       "      <td>-0.367360</td>\n",
       "      <td>-0.221883</td>\n",
       "      <td>0.819427</td>\n",
       "      <td>-0.745828</td>\n",
       "      <td>-0.765522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.786979</td>\n",
       "      <td>-0.911642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64kt_r_med</th>\n",
       "      <td>0.341410</td>\n",
       "      <td>0.198403</td>\n",
       "      <td>-0.627438</td>\n",
       "      <td>0.589159</td>\n",
       "      <td>0.584951</td>\n",
       "      <td>-0.786979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.732920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind_v_med</th>\n",
       "      <td>0.329425</td>\n",
       "      <td>0.214257</td>\n",
       "      <td>-0.798070</td>\n",
       "      <td>0.773304</td>\n",
       "      <td>0.794760</td>\n",
       "      <td>-0.911642</td>\n",
       "      <td>0.732920</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               damage_USD    deaths     p_min  max_storm_cat  wind_v_max  \\\n",
       "damage_USD       1.000000  0.506317 -0.464473       0.408222    0.400592   \n",
       "deaths           0.506317  1.000000 -0.303482       0.264166    0.268409   \n",
       "p_min           -0.464473 -0.303482  1.000000      -0.938941   -0.959126   \n",
       "max_storm_cat    0.408222  0.264166 -0.938941       1.000000    0.975015   \n",
       "wind_v_max       0.400592  0.268409 -0.959126       0.975015    1.000000   \n",
       "p_med           -0.367360 -0.221883  0.819427      -0.745828   -0.765522   \n",
       "64kt_r_med       0.341410  0.198403 -0.627438       0.589159    0.584951   \n",
       "wind_v_med       0.329425  0.214257 -0.798070       0.773304    0.794760   \n",
       "\n",
       "                  p_med  64kt_r_med  wind_v_med  \n",
       "damage_USD    -0.367360    0.341410    0.329425  \n",
       "deaths        -0.221883    0.198403    0.214257  \n",
       "p_min          0.819427   -0.627438   -0.798070  \n",
       "max_storm_cat -0.745828    0.589159    0.773304  \n",
       "wind_v_max    -0.765522    0.584951    0.794760  \n",
       "p_med          1.000000   -0.786979   -0.911642  \n",
       "64kt_r_med    -0.786979    1.000000    0.732920  \n",
       "wind_v_med    -0.911642    0.732920    1.000000  "
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Isolating storms that have wind-radius data and effect\n",
    "hur_wind = hur_cost.dropna().reset_index(drop=True).select_dtypes(exclude = ['object'])\n",
    "#hur_wind.info()\n",
    "#hur_wind.corr()['damage_USD'].sort_values(ascending = False)\n",
    "\n",
    "# Get 6 strongest correlations\n",
    "strong_corrs = hur_wind.corr()['damage_USD'].apply(abs)\\\n",
    "    .sort_values(ascending = False).index[:8]\n",
    "hur_wind[strong_corrs].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 507 entries, 1 to 727\n",
      "Data columns (total 18 columns):\n",
      "damage_USD        507 non-null float64\n",
      "max_storm_cat     507 non-null int64\n",
      "deaths            507 non-null int64\n",
      "duration          507 non-null float64\n",
      "lat_delta         507 non-null float64\n",
      "lon_delta         507 non-null float64\n",
      "wind_v_max        507 non-null float64\n",
      "wind_v_med        507 non-null float64\n",
      "p_min             507 non-null float64\n",
      "p_med             482 non-null float64\n",
      "34kt_r_max        193 non-null float64\n",
      "34kt_r_med        193 non-null float64\n",
      "50kt_r_max        193 non-null float64\n",
      "50kt_r_med        193 non-null float64\n",
      "64kt_r_max        193 non-null float64\n",
      "64kt_r_med        193 non-null float64\n",
      "damage_imputed    507 non-null int64\n",
      "landfall          507 non-null int64\n",
      "dtypes: float64(14), int64(4)\n",
      "memory usage: 75.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Isolating storms with effect, numeric columns\n",
    "hur_numeric = hur_cost[effect].select_dtypes(exclude=['object'])\n",
    "hur_numeric.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be two model paths going forward:\n",
    " * Train a model only on the features where most storms have data. This would mean excluding the windspeed radii features, for example.\n",
    " * Retain all storm records, and replace the NaN values with imputed data. Options for imputing:\n",
    "  * Impute based on mean/median of that feature\n",
    "  * Bin the storms if they share similar values for other features, and then impute the data based on the mean/median of that bin itself.  \n",
    "  \n",
    "I'll explore the first option intially, where I retain only the features with mostly non-Nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 230 entries, 0 to 229\n",
      "Data columns (total 18 columns):\n",
      "damage_USD        230 non-null float64\n",
      "max_storm_cat     230 non-null int64\n",
      "deaths            230 non-null int64\n",
      "duration          230 non-null float64\n",
      "lat_delta         230 non-null float64\n",
      "lon_delta         230 non-null float64\n",
      "wind_v_max        230 non-null float64\n",
      "wind_v_med        230 non-null float64\n",
      "p_min             230 non-null float64\n",
      "p_med             230 non-null float64\n",
      "34kt_r_max        230 non-null float64\n",
      "34kt_r_med        230 non-null float64\n",
      "50kt_r_max        230 non-null float64\n",
      "50kt_r_med        230 non-null float64\n",
      "64kt_r_max        230 non-null float64\n",
      "64kt_r_med        230 non-null float64\n",
      "damage_imputed    230 non-null int64\n",
      "landfall          230 non-null int64\n",
      "dtypes: float64(14), int64(4)\n",
      "memory usage: 32.4 KB\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # Controlling\n",
    "#hurricanes = hur_numeric.drop(labels = cols_radii, axis = 1).dropna()\n",
    "hurricanes = hur_wind\n",
    "hurricanes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell standardizes all feature values (except for binary features) and then splits the data into a testing group and a holdout group that is retained for later use validating the best model from testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeX(arr):\n",
    "    '''\n",
    "    Accepts an array-like object of feature values.\n",
    "    Returns array-like object of standard scaled feature values.\n",
    "    In this codebook, binary values are in the last two columns.\n",
    "    '''\n",
    "    ssX = StandardScaler()\n",
    "    arr_scaled = ssX.fit_transform(arr)\n",
    "    \n",
    "    # Binary variables are at back indices, replace\n",
    "    # Indexing by position because .fit_transform returns numpy array\n",
    "    arr_scaled[:,-2:] = arr[['damage_imputed', 'landfall']]\n",
    "    \n",
    "    return arr_scaled\n",
    "\n",
    "#Standardize all Features before splitting\n",
    "X_scl = standardizeX(hurricanes.iloc[:,1:])\n",
    "\n",
    "# Create test and holdout groups of data.\n",
    "X_trn_scl, X_holdout_scl, y_trn, y_holdout = train_test_split(\n",
    "    X_scl, hurricanes.iloc[:,0], test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features after splitting\n",
    "# X_trn_scl = standardizeX(X_trn)\n",
    "# X_holdout_scl = standardizeX(X_holdout)\n",
    "\n",
    "# Create train and validation subgroups of training data .\n",
    "# X_trn, X_holdout, y_trn, y_holdout = train_test_split(\n",
    "#     hurricanes.iloc[:,1:], hurricanes.iloc[:,0], test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Left Skewed Data\n",
    "Looking at a distribution of the storm damage, we see that the data is extremely left-skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating log scale y, may be better than standard y.\n",
    "y_trn_log = np.log10(y_trn+1)\n",
    "y_holdout_log = np.log10(y_holdout+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''       SETTING X AND Y TRAINING AND TESTING VALUES HERE        '''\n",
    "#All features, logarithmic y\n",
    "X_trn_scl = X_trn_scl\n",
    "y_trn = y_trn_log\n",
    "y_holdout = y_holdout_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above data set, I'm going to experiment with general linear regression, then move on to Ridge, Lasso, and RidgeCV.  \n",
    "\n",
    "Linear Regression Scores:\n",
    "* all x, y:     0.1653\n",
    "* all x, log y: 0.4375\n",
    "* all x + wind data, y: 0.3671\n",
    "* all x + wind data, logy: 0.611\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6114949408635567"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very basic regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_trn_scl, y_trn)\n",
    "lr.score(X_trn_scl, y_trn)\n",
    "\n",
    "#Outputs R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-354-fe2bce5615dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdegree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn_scl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn_scl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_holdout_scl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    487\u001b[0m         X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n\u001b[1;32m    488\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             copy=self.copy_X, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     X = check_array(X, copy=copy, accept_sparse=['csr', 'csc'],\n\u001b[0;32m--> 168\u001b[0;31m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# everything is finite; fall back to O(n) space np.isfinite to prevent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# false positives from overflow in sum method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n\u001b[0m\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m     34\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     35\u001b[0m          initial=_NoValue):\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step through degrees from 0 to 9 and store the training and test (generalization) error.\n",
    "train_error = np.empty(10)\n",
    "test_error = np.empty(10)\n",
    "for degree in range(10):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_trn_scl, y_trn)\n",
    "    train_error[degree] = mean_squared_error(y_trn, est.predict(X_trn_scl))\n",
    "    test_error[degree] = mean_squared_error(y_holdout, est.predict(X_holdout_scl))\n",
    "\n",
    "# Plot the training and test errors against degree\n",
    "plt.figure(dpi=100, figsize=(8,4))\n",
    "plt.plot(np.arange(10), train_error, color='blue', label='train')\n",
    "plt.plot(np.arange(10), test_error, color='red', label='test')\n",
    "#plt.ylim((0.0, 10))\n",
    "plt.ylabel('log(mean squared error)')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_GSCV(X,y, use_model = Lasso, scoring = 'neg_mean_squared_error', cv=10):\n",
    "    '''\n",
    "    Trains a regression model on provided data using k-fold cross validation through grid search.\n",
    "    Paramters:\n",
    "        X: np.array() or pd.DataFrame(): feature values data set\n",
    "        y: np.array() or pd.Series(): target values data set\n",
    "        use_model: sklearn.linearmodels. : model to train\n",
    "        scoring: str: method for assessing error\n",
    "        cv: number of cross-validation sets to use\n",
    "    Returns:\n",
    "        best_err: float: lowest error value achieved\n",
    "        best_params: dict: hyperparameters tuned by model\n",
    "    '''\n",
    "    model = use_model()\n",
    "    parameters = {'alpha': np.linspace(1e0, 1e1,100), 'fit_intercept': [True,False]}\n",
    "    grid = GridSearchCV(model,parameters, cv=10, scoring='neg_mean_squared_error', n_jobs=1)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    return grid.score(X,y), grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso scores:\n",
    "* All features, y: -1.0452361716018397e+20 {'alpha': 367676767.6767677, 'fit_intercept': True}\n",
    "* All features, log y: -5.942746560577917 {'alpha': 0.021003098109810985, 'fit_intercept': True}\n",
    "* All features+wind, y: -1.8282220305331336e+20 {'alpha': 2212121212.121212, 'fit_intercept': True}\n",
    "* All features+wind, logy: -6.011312356167444 {'alpha': 0.08181818181818182, 'fit_intercept': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.011312356167444 {'alpha': 0.08181818181818182, 'fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "lasso_r2, lasso_err, lasso_params = model_GSCV(X_trn_scl, y_trn, Lasso)\n",
    "print(lasso_err, lasso_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge scores:\n",
    "* All features, y: -1.1797184174786937e+20 -1.1953899575459783e+20 {'alpha': 10000.0, 'fit_intercept': True}\n",
    "* All features, log y: -6.443891325810198 {'alpha': 9.0, 'fit_intercept': True}\n",
    "* All features + wind, y: -1.4131373036008315e+20 -1.6711969901985353e+20 {'alpha': 354.54545454545456, 'fit_intercept': True}\n",
    "* All features + wind, log y: -5.09587338893752 -6.039801258028158 {'alpha': 5.090909090909091, 'fit_intercept': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.09587338893752 -6.039801258028158 {'alpha': 5.090909090909091, 'fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "ridge_r2, ridge_err, ridge_params = model_GSCV(X_trn_scl, y_trn, Ridge)\n",
    "print(ridge_r2, ridge_err, ridge_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LassoCV and RidgeCV\n",
    "On to Ridge and Lasso with cross-validation. sklearn [RidgeCV()](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "and [LassoCV()](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
    "documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RidgeCV Scores:\n",
    "* all x, y   : r2: 0.11678957501787333 lambda:  154.54545454545456  intercept: 1487266100.0822582\n",
    "* all x, ylog: r2: 0.42701362725593883 lambda:  5.1  intercept: 3.7536863966703358\n",
    "* all x + wind, y: \n",
    "r2: 0.24486469500507693 lambda:  356.5656565656566  intercept: 2921218969.191903  \n",
    "* all x + wind, log y: r2: 0.6059790492056177 lambda:  5.090909090909091  intercept: 2.658459849850538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.6059790492056177 lambda:  5.090909090909091  intercept: 2.658459849850538\n"
     ]
    }
   ],
   "source": [
    "fit_rcv = RidgeCV(alphas = np.linspace(1e0, 1e1,100),\n",
    "              fit_intercept= [True,False],\n",
    "              cv = 10,\n",
    "             scoring = 'neg_mean_squared_error')\n",
    "\n",
    "fit_rcv.fit(X_trn_scl, y_trn)\n",
    "#cv_values = fit_rcv.cv_values_\n",
    "score_rcv = fit_rcv.score(X_trn_scl, y_trn)\n",
    "print('r2:', score_rcv, 'lambda: ',fit_rcv.alpha_, ' intercept:', fit_rcv.intercept_)\n",
    "#compute_avg_score(cv_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LassoCV Scores:\n",
    "* all x, y:     r2: 0.06888981280302398 lambda:  2454545454.5454545  intercept: 1755815677.8446949\n",
    "* all x, log y: r2: 0.42657817853493596 lambda:  0.04727272727272728  intercept: 4.147931360310995\n",
    "* all x + wind, y: r2: 0.25467451211909387 lambda:  2181818181.818182  intercept: 3012415955.220771\n",
    "* all x + wind, log y: r2: 0.5901793227151957 lambda:  0.08181818181818182  intercept: 2.609131442053207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.5901793227151957 lambda:  0.08181818181818182  intercept: 2.609131442053207\n"
     ]
    }
   ],
   "source": [
    "fit_lcv = LassoCV(\n",
    "            alphas = np.linspace(1e-2,1e-1, 100),\n",
    "            #alphas = np.linspace(1e-2, 1e3,100),\n",
    "              fit_intercept = [True,False],\n",
    "              max_iter = 1000,\n",
    "                cv = 10)\n",
    "\n",
    "fit_lcv.fit(X_trn_scl, y_trn)\n",
    "score_lcv = fit_lcv.score(X_trn_scl, y_trn)\n",
    "print('r2:', score_lcv, 'lambda: ',fit_lcv.alpha_, ' intercept:', fit_lcv.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.04988549874727"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current LCV MSE\n",
    "np.mean(fit_lcv.mse_path_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model so Far\n",
    "Of the models tested so far, I get the best results using this model and this combination of features. Here, we'll get that model's performance on the holdout set and take a look at our parameter (beta) values when training on the whole set. \n",
    "\n",
    "Take a closer look at what type of error metric you're using. [Link](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "Check out cross_validate() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y1, y2):\n",
    "    \"\"\"\n",
    "    Accepts two arrays: estimated and true values\n",
    "    Returns the mean squared error of the prediction.\n",
    "    \"\"\"\n",
    "    return np.mean((y1-y2)**2)\n",
    "\n",
    "def rmse(y1, y2):\n",
    "    \"\"\"\n",
    "    Accepts two arrays: estimated and true values\n",
    "    Returns the mean squared error of the prediction.\n",
    "    \"\"\"\n",
    "    return (mse(y1, y2)/len(y1))**0.5\n",
    "\n",
    "def r2(y_hat, y):\n",
    "    \"\"\"\n",
    "    Accepts two arrays: estimated and true values\n",
    "    Returns the mean squared error of the prediction.\n",
    "    \"\"\"\n",
    "    y_bar = np.mean(y)\n",
    "    ss_res = np.sum((y_hat-y)**2)\n",
    "    ss_tot = np.sum((y-y_bar)**2)\n",
    "    return 1.0 - (ss_res/ss_tot)\n",
    "    \n",
    "def print_metrics(y_pred,y_actual, metrics = [mse, rmse]):\n",
    "    \"\"\"\n",
    "    Wrapper function: accepts two arrays of y values\n",
    "    Returns: lst[num, num]: list of error values\n",
    "    \"\"\"\n",
    "    print('Model performance on holdout set:')\n",
    "    print('MSE:  ', mse(10**y_pred,10**y_actual))\n",
    "    print('RMSE: ', rmse(10**y_pred,10**y_actual))\n",
    "    print('R2:   ', r2(y_pred, y_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Performing Models:\n",
    "* All x, y: Linear Regression (could also use LassoCV)\n",
    "  * Model performance on holdout set:\n",
    "  * MSE:   4.646294923413099e+19\n",
    "  * RMSE:  692097880.8236055\n",
    "  * R2:    0.20645149748487068  \n",
    "\n",
    "* All x, logy: Linear Regression\n",
    " * Model performance on holdout set:\n",
    " * MSE:   9.443324876607267e+27\n",
    " * RMSE:  9866806206026.924\n",
    " * R2:    0.3957244792898038  \n",
    "\n",
    "* All x+wind, y: Linear Regression\n",
    "  * MSE:   2.0887221629438414e+20\n",
    "  * RMSE:  2130891915.190853\n",
    "  * R2:    0.403634171479184\n",
    "* All x+win, log y: RidgeCV\n",
    "  * MSE:   6.785242370532622e+19\n",
    "  * RMSE:  1214517471.7747877\n",
    "  * R2:    0.6800187544837277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on holdout set:\n",
      "MSE:   4.599176647195184e+20\n",
      "RMSE:  3161994639.874999\n",
      "R2:    0.683870386296244\n"
     ]
    }
   ],
   "source": [
    "# Set desired data\n",
    "X_to_use = X_trn_scl\n",
    "y_to_use = y_trn\n",
    "X_to_holdout= X_holdout_scl\n",
    "y_to_holdout = y_holdout\n",
    "\n",
    "# Fit data, make predictions, get error values\n",
    "# champion_model = LassoCV(alphas = np.linspace(1e-6, 1e6,1000),\n",
    "#               fit_intercept= [True,False],\n",
    "#               max_iter = 10000,\n",
    "#                 cv = 10)\n",
    "\n",
    "# champion_model = RidgeCV(alphas = [5.09],\n",
    "#               fit_intercept= [True,False],\n",
    "#               cv = 10,\n",
    "#              scoring = 'neg_mean_squared_error')\n",
    "champion_model = LinearRegression()\n",
    "\n",
    "champion_model.fit(X_to_use, y_to_use)\n",
    "y_holdout_predictions = champion_model.predict(X_to_holdout)\n",
    "print_metrics(y_holdout_predictions, y_to_holdout)\n",
    "\n",
    "#print(champion_model.score(X_to_holdout, y_to_holdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41676914570094176\n",
      "max_storm_cat -714436207.5284172\n",
      "deaths 5406083657.217589\n",
      "duration 382749867.6861561\n",
      "lat_delta -1262736903.413708\n",
      "lon_delta -36187787.27498712\n",
      "wind_v_max -3675219324.492042\n",
      "wind_v_med -1137541309.4046266\n",
      "p_min -9244591343.71644\n",
      "p_med -2228481946.399686\n",
      "34kt_r_max 306614229.01608276\n",
      "34kt_r_med -1416929900.1549888\n",
      "50kt_r_max -2754772033.8343506\n",
      "50kt_r_med -1514007388.0506463\n",
      "64kt_r_max 3112801908.2574987\n",
      "64kt_r_med 1583088541.1034439\n",
      "damage_imputed -281305069.09205616\n",
      "landfall 603798871.8825649\n"
     ]
    }
   ],
   "source": [
    "# Train champion model on full dataset to get the parameters\n",
    "X = standardizeX(hurricanes.iloc[:,1:])\n",
    "#y = np.log10(hurricanes.iloc[:,0]+1)\n",
    "y = hurricanes.iloc[:,0]\n",
    "champion_model_all = champion_model\n",
    "\n",
    "champion_model_all.fit(X,y)\n",
    "champion_r2 = champion_model_all.score(X,y)\n",
    "parameters = champion_model_all.coef_\n",
    "print(champion_r2)\n",
    "\n",
    "for idx, val in enumerate(parameters):\n",
    "    print(hurricanes.iloc[:,1:].columns[idx], val)\n",
    "#print(parameters)\n",
    "#coefficients from pipeline object: est.setps[-1][1].coef_ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models - Gamma Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:302: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n",
      "  DomainWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/families/links.py:300: RuntimeWarning: divide by zero encountered in power\n",
      "  return np.power(z, 1. / self.power)\n",
      "/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:696: RuntimeWarning: invalid value encountered in true_divide\n",
      "  resid_dev = -np.log(endog_mu) + (endog - mu) / mu\n",
      "/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:754: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sum(resid / self.family.variance(mu)) / self.df_resid\n",
      "/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:131: RuntimeWarning: invalid value encountered in multiply\n",
      "  return 1. / (self.link.deriv(mu)**2 * self.variance(mu))\n",
      "/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:1129: RuntimeWarning: invalid value encountered in multiply\n",
      "  - self._offset_exposure)\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge in Linear Least Squares",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-405-5fd43689d204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgamma_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamilies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgamma_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, maxiter, method, tol, scale, cov_type, cov_kwds, use_t, full_output, disp, max_start_irls, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             return self._fit_irls(start_params=start_params, maxiter=maxiter,\n\u001b[1;32m   1011\u001b[0m                                   \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                                   cov_kwds=cov_kwds, use_t=use_t, **kwargs)\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optim_hessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optim_hessian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36m_fit_irls\u001b[0;34m(self, start_params, maxiter, tol, scale, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0mwlsendog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                     \u001b[0mwlsexog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                     self.weights).fit(method=wls_method)\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0mlin_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwls_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0mlin_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset_exposure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/statsmodels/regression/_tools.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             params, _, _, _ = np.linalg.lstsq(self.wexog, self.wendog,\n\u001b[0;32m---> 93\u001b[0;31m                                               rcond=-1)\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mfitted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DDd->Ddid'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'ddd->ddid'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_lstsq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m     \u001b[0;31m# remove the axis we added\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_lstsq\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_lstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVD did not converge in Linear Least Squares\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge in Linear Least Squares"
     ]
    }
   ],
   "source": [
    "gamma_model = sm.GLM(y, X, family=sm.families.Gamma())\n",
    "gamma_results = gamma_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
